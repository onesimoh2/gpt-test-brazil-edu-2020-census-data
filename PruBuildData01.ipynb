{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This first cell will read an Excel file (single book) and flat their multiple hierarchical columns and the first column that contains space indentation in each row. Then it will create embeddings for each title rows and save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "    Yield successive n-sized chunks from a list.\n",
    "\n",
    "    Parameters:\n",
    "    - lst (list): The input list to be chunked.\n",
    "    - n (int): The size of each chunk.\n",
    "\n",
    "    Yields:\n",
    "    - chunk (list): A chunk of size n from the input list.\n",
    "\n",
    "    This function iterates over the input list and yields successive chunks of size n.\n",
    "    If the length of the list is not divisible evenly by n, the last chunk may be smaller\n",
    "    than n. This function is useful for dividing a large list into smaller, more manageable\n",
    "    chunks for processing or analysis.\n",
    "\n",
    "    Example:\n",
    "    >>> lst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    >>> chunks = chunk_list(lst, 3)\n",
    "    >>> for chunk in chunks:\n",
    "    ...     print(chunk)\n",
    "    [1, 2, 3]\n",
    "    [4, 5, 6]\n",
    "    [7, 8, 9]\n",
    "    [10]\n",
    "    \"\"\"\n",
    "def chunk_list(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \").replace(\".\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    res = client.embeddings.create(input = [text], model=model)\n",
    "    emb = res.data[0].embedding\n",
    "    return emb\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    num = np.dot(A,B)\n",
    "    sum1 = sum(map(lambda x: x * x, A))\n",
    "    sum2 = sum(map(lambda x: x * x, B))\n",
    "    den = np.sqrt(sum1) * np.sqrt(sum2)\n",
    "    if(den == 0.0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(num) / den\n",
    "\n",
    "def load_environment_variables(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        settings = json.load(file)\n",
    "        for key, value in settings.items():\n",
    "            os.environ[key] = value\n",
    "\n",
    "\n",
    " \"\"\"\n",
    "    Process hierarchical data and generate a list of rows with hierarchical structure.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list): A list containing strings representing hierarchical data.\n",
    "\n",
    "    Returns:\n",
    "    - rows (list): A list of strings representing rows with hierarchical structure.\n",
    "\n",
    "    This function takes a list of strings representing hierarchical data where indentation\n",
    "    indicates the hierarchy levels. It processes the data and generates a list of rows with\n",
    "    hierarchical structure, where each row represents a level in the hierarchy. The rows\n",
    "    are constructed by concatenating the hierarchical elements separated by dashes (-).\n",
    "\n",
    "    Example:\n",
    "    >>> data = [\n",
    "    ...     'Áreas Gerais, Áreas Detalhadas e Tipo de Cursos ',\n",
    "    ...     '  Ciência da educação',\n",
    "    ...     '    Processos escolares',\n",
    "    ...     '    Psicopedagogia',\n",
    "    ...     '  Formação de professores de educação infantil',\n",
    "    ...     '    Educação infantil formação de professor'\n",
    "    ... ]\n",
    "    >>> result = q_process_hierarchy(data)\n",
    "    >>> print(result)\n",
    "    ['Áreas Gerais, Áreas Detalhadas e Tipo de Cursos ',\n",
    "     'Áreas Gerais, Áreas Detalhadas e Tipo de Cursos -Ciência da educação',\n",
    "     'Áreas Gerais, Áreas Detalhadas e Tipo de Cursos -Ciência da educação-Processos escolares',\n",
    "     'Áreas Gerais, Áreas Detalhadas e Tipo de Cursos -Ciência da educação-Psicopedagogia',\n",
    "     'Áreas Gerais, Áreas Detalhadas e Tipo de Cursos -Formação de professores de educação infantil',\n",
    "     'Áreas Gerais, Áreas Detalhadas e Tipo de Cursos -Formação de professores de educação infantil-Educação infantil formação de professor']\n",
    "    \"\"\"\n",
    "def q_process_hierarchy(data):\n",
    "    l = len(data)\n",
    "\n",
    "    rows = []\n",
    "    queue = deque()\n",
    "    for item in data:\n",
    "        if pd.isna(item):\n",
    "            continue\n",
    "        indent_level = len(item) - len(item.lstrip())\n",
    "        while queue and queue[-1][1] >= indent_level:\n",
    "            queue.pop()\n",
    "        queue.append((item.strip(), indent_level))\n",
    "        rows.append(\"-\".join([row[0] for row in queue]))\n",
    "        #print(\"-\".join([row[0] for row in queue]))\n",
    "    return rows\n",
    "\n",
    "local_settings_path = 'local.settings.json'\n",
    "load_environment_variables(local_settings_path)\n",
    "\n",
    "op_key = os.environ[\"API_KEY\"]\n",
    "pddf = pd.read_excel('data/Libro2.xlsx', index_col=None, header=[0, 1, 2, 3])\n",
    "pddf.columns = pddf.columns.to_flat_index()\n",
    "# Remove substrings containing 'Unnamed: 0_level_'\n",
    "pddf.columns = [(s[0],) + tuple(x for x in s[1:] if 'Unnamed:' not in x and 'Unnamed: 1_level_' not in x) for s in pddf.columns]\n",
    "\n",
    "c1_nam = pddf.columns[0]\n",
    "pddf = pddf.dropna(how='all')\n",
    "\n",
    "expanded_df = q_process_hierarchy(pddf.iloc[:, 0])\n",
    "\n",
    "l = len(expanded_df)\n",
    "\n",
    "pddf.iloc[:, 0] = expanded_df\n",
    "\n",
    "# emb_text = [chunk.get(\"('Áreas Gerais, Áreas Detalhadas e Tipo de Cursos ',)\", \"\").strip() for chunk in list_of_dicts] \n",
    "# df_mod[\"emb_text\"] = emb_text\n",
    "\n",
    "# Add the 'Id' column at the beginning\n",
    "#df_mod .insert(0, 'Id', range(1, len(df) + 1))\n",
    "\n",
    "\n",
    "pddf.to_excel('data/Libro2Flat.xlsx')\n",
    "pddf.to_csv('data/cLibro2Flat.csv', encoding='iso-8859-1', index=False)\n",
    "\n",
    "df = pd.read_csv(\"data/cLibro2Flat.csv\", encoding='latin-1')\n",
    "json_per_row = df.to_json(indent=2, orient='records', force_ascii=False)\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 1\n",
    "list_of_dicts = json.loads(json_per_row)\n",
    "\n",
    "chunks = list(chunk_list(list_of_dicts, chunk_size))\n",
    "\n",
    "# Convert list of dictionaries to JSON strings\n",
    "json_chunks = [json.dumps(chunk, ensure_ascii=False) for chunk in chunks]\n",
    "\n",
    "# Create DataFrame from JSON chunks\n",
    "df_mod = pd.DataFrame({'data_text': json_chunks})\n",
    "\n",
    "#create text for embeddings  ('Áreas Gerais, Áreas Detalhadas e Tipo de Cursos ',)\n",
    "emb_text = [chunk.get(\"('Áreas Gerais, Áreas Detalhadas e Tipo de Cursos ',)\", \"\").strip() for chunk in list_of_dicts] \n",
    "\n",
    "#emb_text = [chunk.get(\"Áreas Gerais, \", \"\").strip() + \" \"  + chunk.get(\"Áreas Detalhadas\", \"\").strip() + \" \" + chunk.get(\"Tipo de Cursos\", \"\").strip() + \" \"  for chunk in list_of_dicts] #+ chunk.get(\"Áreas Detalhadas\", \"\")\n",
    "df_mod[\"emb_text\"] = emb_text\n",
    "\n",
    "# Rename the column to 'data_chunks'\n",
    "#df = df.rename(columns={0: 'data_chunks'})\n",
    "\n",
    "\n",
    "\n",
    "# Add the 'embedding' column with empty values\n",
    "#df['embedding'] = 0\n",
    "embedding_model = \"text-embedding-3-small\" #\"text-embedding-ada-002\"\n",
    "api_key = op_key\n",
    "client = OpenAI(api_key=api_key)\n",
    "#df_mod ['embeddings'] = df_mod.data_text.apply(lambda x: get_embedding(x, embedding_model))\n",
    "df_mod ['embeddings'] = df_mod.emb_text.apply(lambda x: get_embedding(x, embedding_model))\n",
    "df_mod.to_csv('data/Libro2FlatWithEmb.csv', encoding='iso-8859-1', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This second cell will read the file with the embeddings produced in the previous cell and will find those similar (or relevant to answer) a user question. Then it saves the results into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "\n",
    "\n",
    "def chunk_list(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \").replace(\".\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    # text = text.replace(\"{\", \" \")\n",
    "    # text = text.replace(\"}\", \" \")\n",
    "    res = client.embeddings.create(input = [text], model=model)\n",
    "    emb = res.data[0].embedding\n",
    "    #emb = res['data'][0]['embedding']\n",
    "    return emb\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    num = np.dot(A,B)\n",
    "    sum1 = sum(map(lambda x: x * x, A))\n",
    "    sum2 = sum(map(lambda x: x * x, B))\n",
    "    den = np.sqrt(sum1) * np.sqrt(sum2)\n",
    "    if(den == 0.0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(num) / den\n",
    "\n",
    "\n",
    "local_settings_path = 'local.settings.json'\n",
    "load_environment_variables(local_settings_path)\n",
    "op_key = os.environ[\"API_KEY\"]\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\" #\"text-embedding-ada-002\"\n",
    "api_key = op_key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "df_mod = pd.read_csv(\"data/Libro2FlatWithEmb.csv\", encoding='latin-1')\n",
    "\n",
    "#query = \"Tell me the number for Privada  Letras línguas estrangeiras clássicas formação de professor  \"\n",
    "query = \"tell me the overal total number\"\n",
    "#query = \"tell me the Educação total number\"\n",
    "#query = \"tell me the average number for  Formação de professores de letras\"\n",
    "\n",
    "\n",
    "query_embedding = get_embedding(query, embedding_model)  #Find embeddings directly from the query\n",
    "\n",
    "# Obtain the similarity of each fact to the query\n",
    "#ada_embedding = df.embeddings.apply(eval).apply(np.array)\n",
    "\n",
    "ada_embedding = df_mod ['embeddings']\n",
    "i = 0 \n",
    "for row_emb in ada_embedding:\n",
    "    # Parse the string into a list of floats\n",
    "    doubles_list = ast.literal_eval(row_emb)\n",
    "    nprow_emb = np.array(doubles_list)\n",
    "    num = cosine_similarity(query_embedding, nprow_emb)\n",
    "    df_mod.loc[i,'similarity'] = 1 / (2 - num)  #(1 / 1 + distance)  monotonically decreasing\n",
    "    i = i + 1\n",
    "\n",
    "print(df_mod['similarity'])\n",
    "\n",
    "similarity_array = df_mod['similarity'].values\n",
    "quantiles = np.percentile(similarity_array, [25, 50, 75])\n",
    "\n",
    "num = quantiles[2]\n",
    "\n",
    "selected_rows = df_mod[df_mod['similarity'] >= num]\n",
    "\n",
    "sel_row_emb_text = selected_rows['emb_text']\n",
    "sel_row_emb_text.to_csv('data/sel_row_emb_text.csv')\n",
    "\n",
    "print(selected_rows['emb_text'])\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_mod.to_csv('data/chunk_data_and_embeddings.csv', encoding='iso-8859-1', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
